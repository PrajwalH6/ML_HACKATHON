{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73031dcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# RL Agent Training\\n\",\n",
    "    \"Train Q-learning agent with HMM oracle guidance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.env.hangman_env import HangmanEnv\\n\",\n",
    "    \"from src.hmm.oracle import HMMOracle\\n\",\n",
    "    \"from src.hmm.emissions import EmissionBuilder\\n\",\n",
    "    \"from src.rl.q_learning import QLearningAgent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Data and Create Oracle\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load preprocessed data\\n\",\n",
    "    \"with open('../data/processed/words_by_length.pkl', 'rb') as f:\\n\",\n",
    "    \"    words_by_length = pickle.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load HMM emissions\\n\",\n",
    "    \"emissions = EmissionBuilder.load('../models/hmm/emissions.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create oracle\\n\",\n",
    "    \"oracle = HMMOracle(emissions, words_by_length)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare training words\\n\",\n",
    "    \"all_words = []\\n\",\n",
    "    \"for words in words_by_length.values():\\n\",\n",
    "    \"    all_words.extend(words)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training vocabulary: {len(all_words)} words\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Initialize Agent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"agent = QLearningAgent(\\n\",\n",
    "    \"    learning_rate=0.1,\\n\",\n",
    "    \"    discount_factor=0.95,\\n\",\n",
    "    \"    epsilon=0.3,\\n\",\n",
    "    \"    epsilon_decay=0.9995,\\n\",\n",
    "    \"    epsilon_min=0.01\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Agent initialized with epsilon={agent.epsilon}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Training Loop\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"num_episodes = 5000\\n\",\n",
    "    \"episode_rewards = []\\n\",\n",
    "    \"episode_wins = []\\n\",\n",
    "    \"win_rate_history = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for episode in tqdm(range(num_episodes)):\\n\",\n",
    "    \"    # Sample random word\\n\",\n",
    "    \"    word = random.choice(all_words)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create environment\\n\",\n",
    "    \"    env = HangmanEnv(word, max_lives=6)\\n\",\n",
    "    \"    state = env.reset()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    total_reward = 0\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        # Get HMM probabilities\\n\",\n",
    "    \"        hmm_probs = oracle.get_letter_probs(state['mask'], state['guessed_letters'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Select action\\n\",\n",
    "    \"        valid_actions = env.get_valid_actions()\\n\",\n",
    "    \"        action = agent.select_action(state, hmm_probs, valid_actions, training=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Take step\\n\",\n",
    "    \"        next_state, reward, done, info = env.step(action)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get next HMM probabilities\\n\",\n",
    "    \"        next_hmm_probs = oracle.get_letter_probs(\\n\",\n",
    "    \"            next_state['mask'], \\n\",\n",
    "    \"            next_state['guessed_letters']\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Update agent\\n\",\n",
    "    \"        agent.update(state, action, reward, next_state, done, hmm_probs, next_hmm_probs)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        state = next_state\\n\",\n",
    "    \"        total_reward += reward\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Decay epsilon\\n\",\n",
    "    \"    agent.decay_epsilon()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Track metrics\\n\",\n",
    "    \"    episode_rewards.append(total_reward)\\n\",\n",
    "    \"    episode_wins.append(1 if info.get('win', False) else 0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Compute rolling win rate\\n\",\n",
    "    \"    if episode >= 99:\\n\",\n",
    "    \"        win_rate = sum(episode_wins[-100:]) / 100\\n\",\n",
    "    \"        win_rate_history.append(win_rate)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Log progress\\n\",\n",
    "    \"    if (episode + 1) % 500 == 0:\\n\",\n",
    "    \"        recent_wins = sum(episode_wins[-500:])\\n\",\n",
    "    \"        recent_reward = sum(episode_rewards[-500:]) / 500\\n\",\n",
    "    \"        print(f\\\"\\\\nEpisode {episode+1}: Win Rate={recent_wins/500:.3f}, \\\"\\n\",\n",
    "    \"              f\\\"Avg Reward={recent_reward:.2f}, Epsilon={agent.epsilon:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTraining complete!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Plot Training Curves\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fig, axes = plt.subplots(2, 1, figsize=(12, 10))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Reward curve\\n\",\n",
    "    \"window = 100\\n\",\n",
    "    \"smoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[0].plot(smoothed_rewards, linewidth=2)\\n\",\n",
    "    \"axes[0].set_xlabel('Episode')\\n\",\n",
    "    \"axes[0].set_ylabel('Average Reward')\\n\",\n",
    "    \"axes[0].set_title('Training Reward Curve (100-episode moving average)')\\n\",\n",
    "    \"axes[0].grid(alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Win rate curve\\n\",\n",
    "    \"axes[1].plot(win_rate_history, linewidth=2, color='green')\\n\",\n",
    "    \"axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% target')\\n\",\n",
    "    \"axes[1].set_xlabel('Episode')\\n\",\n",
    "    \"axes[1].set_ylabel('Win Rate')\\n\",\n",
    "    \"axes[1].set_title('Rolling Win Rate (100-episode window)')\\n\",\n",
    "    \"axes[1].legend()\\n\",\n",
    "    \"axes[1].grid(alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFinal 1000 episodes win rate: {sum(episode_wins[-1000:])/1000:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Save Trained Agent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save agent\\n\",\n",
    "    \"Path('../models/rl').mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"agent.save('../models/rl/q_table_final.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save training history\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"history = {\\n\",\n",
    "    \"    'rewards': episode_rewards,\\n\",\n",
    "    \"    'wins': episode_wins,\\n\",\n",
    "    \"    'win_rate_history': win_rate_history\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('../models/rl/training_history.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(history, f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Agent and training history saved!\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.12.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

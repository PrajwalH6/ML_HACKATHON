{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda3df6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Model Evaluation on Test Set\\n\",\n",
    "    \"Evaluate trained agent on 2000 test games and compute final metrics.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.env.hangman_env import HangmanEnv\\n\",\n",
    "    \"from src.hmm.oracle import HMMOracle\\n\",\n",
    "    \"from src.hmm.emissions import EmissionBuilder\\n\",\n",
    "    \"from src.rl.q_learning import QLearningAgent\\n\",\n",
    "    \"from src.utils.data_loader import load_test_words\\n\",\n",
    "    \"from src.utils.metrics import MetricsCalculator\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Test Data and Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load test words\\n\",\n",
    "    \"all_test_words = load_test_words('../data/raw/text.txt')\\n\",\n",
    "    \"test_words = all_test_words[:2000]  # Use first 2000\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(test_words)} test words\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load preprocessed data\\n\",\n",
    "    \"with open('../data/processed/words_by_length.pkl', 'rb') as f:\\n\",\n",
    "    \"    words_by_length = pickle.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load HMM\\n\",\n",
    "    \"emissions = EmissionBuilder.load('../models/hmm/emissions.pkl')\\n\",\n",
    "    \"oracle = HMMOracle(emissions, words_by_length)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load agent\\n\",\n",
    "    \"agent = QLearningAgent()\\n\",\n",
    "    \"agent.load('../models/rl/q_table_final.pkl')\\n\",\n",
    "    \"agent.epsilon = 0.0  # No exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Models loaded successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Run Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"metrics_calc = MetricsCalculator()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for word in tqdm(test_words, desc=\\\"Evaluating\\\"):\\n\",\n",
    "    \"    env = HangmanEnv(word, max_lives=6)\\n\",\n",
    "    \"    state = env.reset()\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        hmm_probs = oracle.get_letter_probs(state['mask'], state['guessed_letters'])\\n\",\n",
    "    \"        valid_actions = env.get_valid_actions()\\n\",\n",
    "    \"        action = agent.select_action(state, hmm_probs, valid_actions, training=False)\\n\",\n",
    "    \"        state, reward, done, info = env.step(action)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    metrics_calc.add_game(\\n\",\n",
    "    \"        word=word,\\n\",\n",
    "    \"        won=info.get('win', False),\\n\",\n",
    "    \"        wrong_guesses=env.wrong_guesses,\\n\",\n",
    "    \"        repeated_guesses=env.repeated_guesses\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nEvaluation complete!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Compute and Display Metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"metrics = metrics_calc.compute_metrics()\\n\",\n",
    "    \"df = metrics_calc.get_dataframe()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"print(\\\"EVALUATION RESULTS\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"print(f\\\"Total Games: {metrics['total_games']}\\\")\\n\",\n",
    "    \"print(f\\\"Wins: {metrics['wins']}\\\")\\n\",\n",
    "    \"print(f\\\"Losses: {metrics['losses']}\\\")\\n\",\n",
    "    \"print(f\\\"Success Rate: {metrics['success_rate']*100:.2f}%\\\")\\n\",\n",
    "    \"print(f\\\"Total Wrong Guesses: {metrics['total_wrong_guesses']}\\\")\\n\",\n",
    "    \"print(f\\\"Total Repeated Guesses: {metrics['total_repeated_guesses']}\\\")\\n\",\n",
    "    \"print(f\\\"Avg Wrong/Game: {metrics['avg_wrong_per_game']:.2f}\\\")\\n\",\n",
    "    \"print(f\\\"Avg Repeated/Game: {metrics['avg_repeated_per_game']:.2f}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nFINAL SCORE: {metrics['final_score']:.2f}\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Visualize Results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Win/Loss pie chart\\n\",\n",
    "    \"axes[0, 0].pie([metrics['wins'], metrics['losses']], \\n\",\n",
    "    \"               labels=['Wins', 'Losses'],\\n\",\n",
    "    \"               autopct='%1.1f%%',\\n\",\n",
    "    \"               colors=['#2ecc71', '#e74c3c'])\\n\",\n",
    "    \"axes[0, 0].set_title(f\\\"Success Rate: {metrics['success_rate']*100:.1f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Wrong guesses distribution\\n\",\n",
    "    \"axes[0, 1].hist(df['wrong_guesses'], bins=range(0, 8), edgecolor='black')\\n\",\n",
    "    \"axes[0, 1].set_xlabel('Wrong Guesses')\\n\",\n",
    "    \"axes[0, 1].set_ylabel('Frequency')\\n\",\n",
    "    \"axes[0, 1].set_title('Wrong Guesses Distribution')\\n\",\n",
    "    \"axes[0, 1].grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Repeated guesses distribution\\n\",\n",
    "    \"if df['repeated_guesses'].max() > 0:\\n\",\n",
    "    \"    axes[1, 0].hist(df['repeated_guesses'], \\n\",\n",
    "    \"                   bins=range(0, df['repeated_guesses'].max()+2), \\n\",\n",
    "    \"                   edgecolor='black')\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    axes[1, 0].text(0.5, 0.5, 'No repeated guesses!', \\n\",\n",
    "    \"                   ha='center', va='center', fontsize=14)\\n\",\n",
    "    \"axes[1, 0].set_xlabel('Repeated Guesses')\\n\",\n",
    "    \"axes[1, 0].set_ylabel('Frequency')\\n\",\n",
    "    \"axes[1, 0].set_title('Repeated Guesses Distribution')\\n\",\n",
    "    \"axes[1, 0].grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Performance by word length\\n\",\n",
    "    \"df['word_length'] = df['word'].apply(len)\\n\",\n",
    "    \"length_stats = df.groupby('word_length')['won'].agg(['sum', 'count'])\\n\",\n",
    "    \"length_stats['success_rate'] = length_stats['sum'] / length_stats['count']\\n\",\n",
    "    \"axes[1, 1].bar(length_stats.index, length_stats['success_rate'], edgecolor='black')\\n\",\n",
    "    \"axes[1, 1].set_xlabel('Word Length')\\n\",\n",
    "    \"axes[1, 1].set_ylabel('Success Rate')\\n\",\n",
    "    \"axes[1, 1].set_title('Success Rate by Word Length')\\n\",\n",
    "    \"axes[1, 1].grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Analyze Failures\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get failed words\\n\",\n",
    "    \"failed_words = df[df['won'] == False]['word'].values\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFailed words: {len(failed_words)}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nSample failed words (first 20):\\\")\\n\",\n",
    "    \"for word in failed_words[:20]:\\n\",\n",
    "    \"    print(word)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Save Results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create output directory\\n\",\n",
    "    \"Path('../reports/results').mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save metrics\\n\",\n",
    "    \"with open('../reports/results/final_metrics.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(metrics, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save detailed results\\n\",\n",
    "    \"df.to_csv('../reports/results/evaluation_results.csv', index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Results saved to reports/results/\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.12.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
